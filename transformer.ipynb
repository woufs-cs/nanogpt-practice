{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d463e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入定义\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "max_len=512\n",
    "d_model=768\n",
    "batch_size=128\n",
    "n_head=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e727d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.token embedding (batch_size,seq_len,vocab_size)-->(batch_size,seq_len,d_model)\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50fc348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Positional embedding (batch_size,seq_len,d_model)-->(batch_size,seq_len,d_model)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,max_len,device):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding=torch.zeros(max_len,d_model).to(device)\n",
    "        self.encoding.requires_grad=False\n",
    "\n",
    "        pos=torch.arange(0,max_len).unsqueeze(1)#(max_len,1)\n",
    "        _2i=torch.arange(0,d_model,step=2)#(d_model/2,)\n",
    "\n",
    "        #赋值PE矩阵\n",
    "        self.encoding[:,0::2]=torch.sin(pos/10000**(_2i/d_model))\n",
    "        self.encoding[:,1::2]=torch.cos(pos/10000**(_2i/d_model))\n",
    "\n",
    "    def forward(self,x):\n",
    "        #x: (batch_size,seq_len,d_model)\n",
    "        seq_len=x.size(1)\n",
    "        return x+self.encoding[:seq_len,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75572c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Multi-head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init(self,d_model,n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.n_head=n_head\n",
    "        self.d_k=d_model//n_head\n",
    "\n",
    "        self.W_q=nn.Linear(d_model,d_model)\n",
    "        self.W_k=nn.Linear(d_model,d_model)\n",
    "        self.W_v=nn.Linear(d_model,d_model)\n",
    "        self.combine=nn.Linear(d_model,d_model)\n",
    "        self.softmax=nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        #这里输入的q,k,v实际上是计算q,k,v的对应输入，encoder中这三个都是x\n",
    "        batch_size,seq_len,d_model=q.shape\n",
    "        n_d=d_model//self.n_head\n",
    "        q=self.W_q(q)\n",
    "        k=self.W_k(k)\n",
    "        v=self.W_v(v)\n",
    "        #切分并转换-->(batch_size,n_head,seq_len,d_model)\n",
    "        q=q.view(batch_size,seq_Len,n_head,n_d).permute(0,2,1,3)\n",
    "        k=k.view(batch_size,seq_Len,n_head,n_d).permute(0,2,1,3)\n",
    "        v=v.view(batch_size,seq_Len,n_head,n_d).permute(0,2,1,3)\n",
    "        #计算注意力得分\n",
    "        scores=torch.matmul(q,k.transpose(-2,-1))/math.sqrt(self.d_k)#(batch_size,n_head,seq_len,seq_len)\n",
    "        #掩码自注意力\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "        #softmax\n",
    "        attn=self.softmax(scores)\n",
    "        context=torch.matmul(attn,v)#(batch_size,n_head,seq_len,d_k)\n",
    "        #拼接并转换\n",
    "        context=context.permute(0,2,1,3).contiguous().view(batch_size,seq_len,d_model)#(batch_size,seq_len,d_model)\n",
    "        output=self.combine(context)#(batch_size,seq_len,d_model)\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Layer Normalization\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,d_model,eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma=nn.Parameter(torch.ones(d_model))\n",
    "        self.beta=nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps=eps\n",
    "\n",
    "    def forward(self,x):\n",
    "        #x: (batch_size,seq_len,d_model)\n",
    "        mean=x.mean(-1,keepdim=True)\n",
    "        var=x.var(-1,unbiased=False,keepdim=True)\n",
    "        out=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.gamma*out+self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411af292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.Feed Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff=2048,drpout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1=nn.Linear(d_model,d_ff)\n",
    "        self.linear2=nn.Linear(d_ff,d_model)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout=nn.Dropout(drpout)\n",
    "    def forward(self,x):\n",
    "        #x: (batch_size,seq_len,d_model)\n",
    "        out=self.linear1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.dropout(out)\n",
    "        out=self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d9d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.Transformer embedding layer\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def ___init__(self,max_len,vocab_size,d_model,device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.token_embedding=TokenEmbedding(vocab_size,d_model)\n",
    "        self.position_encoding=PositionalEncoding(d_model,max_len,device)\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "    def forward(self,x):\n",
    "        token_emb=self.token_embedding(x)\n",
    "        pos_emb=self.position_encoding(x)\n",
    "        emb=token_emb+pos_emb\n",
    "        emb=self.dropout(emb)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.Encoder Layer (batch_size,seq_len,d_model)-->(batch_size,seq_len,d_model)\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,n_head,d_ff=2048,dropout=0.1):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.mha=MultiHeadAttention(d_model,n_head)\n",
    "        self.norm1=LayerNorm(d_model)\n",
    "        self.dropout1=nn.Dropout(dropout)\n",
    "\n",
    "        self.ffn=FeedForward(d_model,d_ff,dropout)\n",
    "        self.norm2=LayerNorm(d_model)\n",
    "        self.dropout2=nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        _x=x\n",
    "        attn_out=self.mha(x,x,x,mask)\n",
    "        attn_out=self.dropout1(attn_out)\n",
    "        x=self.norm1(_x+attn_out)\n",
    "        \n",
    "        _x=x\n",
    "        ffn_out=self.ffn(x)\n",
    "        ffn_out=self.dropout2(ffn_out)\n",
    "        x=self.norm2(_x+ffn_out)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f958c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.Decoder Layer (batch_size,seq_len,d_model)-->(batch_size,seq_len,d_model)\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,n_head,d_ff=2048,dropout=0.1):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.self_mha=MultiHeadAttention(d_model,n_head)\n",
    "        self.norm1=LayerNorm(d_model)\n",
    "        self.dropout1=nn.Dropout(dropout)\n",
    "\n",
    "        self.enc_dec_mha=MultiHeadAttention(d_model,n_head)\n",
    "        self.norm2=LayerNorm(d_model)\n",
    "        self.dropout2=nn.Dropout(dropout)\n",
    "\n",
    "        self.ffn=FeedForward(d_model,d_ff,dropout)\n",
    "        self.norm3=LayerNorm(d_model)\n",
    "        self.dropout3=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x,enc_out,t_mask,s_mask):\n",
    "        _x=x\n",
    "        self_attn_out=self.self_mha(x,x,x,t_mask)\n",
    "        self_attn_out=self.dropout1(self_attn_out)\n",
    "        x=self.norm1(_x+self_attn_out)\n",
    "        \n",
    "        if enc_out is not None:\n",
    "            _x=x\n",
    "            enc_dec_attn_out=self.enc_dec_mha(x,enc_out,enc_out,s_mask)\n",
    "            enc_dec_attn_out=self.dropout2(enc_dec_attn_out)\n",
    "            x=self.norm2(_x+enc_dec_attn_out)\n",
    "\n",
    "        _x=x\n",
    "        ffn_out=self.ffn(x)\n",
    "        ffn_out=self.dropout3(ffn_out)\n",
    "        x=self.norm3(_x+ffn_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ddc04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Transformer Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,num_layers,d_model,n_head,enc_vocab_size,d_ff=2048,dropout=0.1,device='cpu'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding=TransformerEmbedding(max_len,enc_vocab_size,d_model,device)\n",
    "        self.layers=nn.ModuleList([EncoderLayer(d_model,n_head,d_ff,dropout) for _ in range(num_layers)])\n",
    "    def forward(self,x,s_mask):\n",
    "        x=self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,s_mask)\n",
    "        return x\n",
    "    \n",
    "#10. Transformer Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,num_layers,d_model,n_head,dec_vocab_size,d_ff=2048,dropout=0.1,device='cpu'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding=TransformerEmbedding(max_len,dec_vocab_size,d_model,device)\n",
    "        self.layers=nn.ModuleList([DecoderLayer(d_model,n_head,d_ff,dropout) for _ in range(num_layers)])\n",
    "        self.fc=nn.Linear(d_model,dec_vocab_size)\n",
    "    def forward(self,x,enc,t_mask,s_mask):\n",
    "        x=self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,enc,t_mask,s_mask)\n",
    "        dec= self.fc(x)\n",
    "        return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba91109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,src_idx,trg_idx,enc_num_layers,dec_num_layers,d_model,n_head,enc_vocab_size,dec_vocab_size,d_ff=2048,dropout=0.1,device='cpu'):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder=Encoder(enc_num_layers,d_model,n_head,enc_vocab_size,d_ff,dropout,device)\n",
    "        self.decoder=Decoder(dec_num_layers,d_model,n_head,dec_vocab_size,d_ff,dropout,device)\n",
    "        self.src_idx=src_idx\n",
    "        self.trg_idx=trg_idx\n",
    "        self.device=device\n",
    "    def make_casual_mask(self,q,k):#输出一个目标序列和源序列长度的掩码\n",
    "        len_q,len_k=q.size(1),k.size(1)\n",
    "        casual_mask=torch.tril(torch.ones((len_q,len_k),device=self.device)).bool()\n",
    "        return casual_mask\n",
    "    \n",
    "    def make_pad_mask(self, q, k, pad_idx_q, pad_idx_k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        # q_mask 的形状: (Batch, 1, len_q, 1)\n",
    "        q_mask = q.ne(pad_idx_q).unsqueeze(1).unsqueeze(3)\n",
    "        # k_mask 的形状: (Batch, 1, 1, len_k)\n",
    "        k_mask = k.ne(pad_idx_k).unsqueeze(1).unsqueeze(2)\n",
    "        # 广播机制会自动将两个张量扩展到 (Batch, 1, len_q, len_k)\n",
    "        # 然后执行逐元素的逻辑与操作\n",
    "        mask = q_mask & k_mask\n",
    "        return mask\n",
    "\n",
    "    def forward(self,src,tgt,src_mask,tgt_mask):\n",
    "        src_mask=self.make_pad_mask(src,src,self.src_idx,self.src_idx)\n",
    "        trg_mask=self.make_pad_mask(tgt,tgt,self.trg_idx,self.trg_idx)&self.make_casual_mask(tgt,tgt) \n",
    "        src_trg_mask=self.make_pad_mask(tgt,src,self.trg_idx,self.src_idx)\n",
    "        enc_out=self.encoder(src,src_mask)\n",
    "        dec_out=self.decoder(tgt,enc_out,src_trg_mask,trg_mask)\n",
    "        return dec_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
